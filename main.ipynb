{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model Using Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor2Tensor(T2T)\n",
    "an open-source system for training deep learning models in TensorFlow.  \n",
    "A deep learning system consists of datasets, model architectures, optimizers, and hyperparameters.Tensor2tensor ties together all these pieces in coherent and standarized way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create a machine learning model that completes javascript codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a script that parse the text from mlab and split it into two files input.txt and output.txt\n",
    "import json\n",
    "\n",
    "with open(\"wikipedia-JSCoverage.json\") as f1:\n",
    "\tjson_obj = json.loads(f1.read())\n",
    "\t# print(json_obj)\n",
    "\twith open(\"raw.txt\", \"w\") as f2:\n",
    "\t\tfor junk in json_obj[\"junk\"]:\n",
    "\t\t\tfor junklist in junk[\"junk_list\"]:\n",
    "\t\t\t\tf2.write(junklist + \"\\n\");\n",
    "\n",
    "\t# print(json_obj[\"junk\"])\n",
    "\n",
    "\n",
    "with open(\"raw.txt\") as rawf, \\\n",
    "\t open(\"input.txt\", \"w\") as inf, \\\n",
    "\t open(\"output.txt\", \"w\") as outf:\n",
    "\tprev_line = ''\n",
    "\tfor curr_line in rawf:\n",
    "\t\tcurr_line = curr_line.strip()\n",
    "\t\tif len(prev_line) > 0 and len(curr_line) > 0:\n",
    "\t\t\tinf.write(prev_line)\n",
    "\t\t\toutf.write(curr_line)\n",
    "\t\tprev_line = curr_line\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing problem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "\n",
    "@registry.register_problem\n",
    "class JavascriptProblem(text_problems.Text2TextProblem):\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 90,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "    }]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    with open('raw.txt', 'r') as rawfp:\n",
    "      prev_line = ''\n",
    "      for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:       \n",
    "            yield {\n",
    "                \"inputs\": prev_line,\n",
    "                \"targets\": curr_line\n",
    "            }\n",
    "        prev_line = curr_line          \n",
    "\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_javascript():\n",
    "  hparams = transformer.transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_dropout = 0.6\n",
    "  hparams.layer_prepostprocess_dropout = 0.6\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_javascript_range(rhp):\n",
    "  rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "  rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "  rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --problem=languagemodel\n",
    "\n",
    "pip install tensor2tensor && t2t-trainer \\\n",
    "--generate_data \\\n",
    "--data_dir=~/t2t_data \\\n",
    "--output_dir=~/t2t_train/mnist \\\n",
    "--problem=languagemodel_ptb10k \\\n",
    "--model=transformer \\\n",
    "--hparams_set=transformer_small \\\n",
    "--train_steps=200 \\\n",
    "--eval_steps=100\n",
    "\n",
    "t2t-trainer \\\n",
    "  --generate_data \\\n",
    "  --data_dir=~/_programming/major_project/deep_learning/t2t_data \\\n",
    "  --output_dir=~/_programming/major_project/deep_learning/t2t_train/mnist \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_small \\\n",
    "  --train_steps=1000 \\\n",
    "  --eval_steps=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t-decoder \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --problem=$PROBLEM \\\n",
    "  --model=$MODEL \\\n",
    "  --hparams_set=$HPARAMS \\\n",
    "  --output_dir=$TRAIN_DIR \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE \\\n",
    "  --decode_to_file=translation.en\n",
    "\n",
    "# run this, before running make decode_this.txt with Hello, world etc.\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$HOME/t2t_data \\\n",
    "  --problem=languagemodel_ptb10k \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_small \\\n",
    "  --output_dir=$HOME/t2t_train/languagemodel_ptb10k/transformer-transformer_small \\\n",
    "  --decode_hparams=\"beam_size=4,alpha=0.4\" \\\n",
    "  --decode_from_file=$HOME/t2t_data/decode_this.txt \\\n",
    "  --decode_to_file=translation.en\n",
    "\n",
    "# see the file\n",
    "cat translation.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
